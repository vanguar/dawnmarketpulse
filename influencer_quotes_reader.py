# influencer_quotes_reader.py
# v2.0 ‚Äì 10-Jun-2025
#
# –ó–∞–±–∏—Ä–∞–µ–º —Å–≤–µ–∂–∏–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è –∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º GPT –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞.
# –ò–∑–º–µ–Ω–µ–Ω–∏–µ v2.0: –ü–æ–ª–Ω—ã–π –æ—Ç–∫–∞–∑ –æ—Ç googletrans –≤ –ø–æ–ª—å–∑—É –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–µ—Ä–µ–∑ OpenAI GPT.

import os
import time
import html
import re
import requests
import openai
from datetime import datetime, timedelta
from custom_logger import log

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GPT ---
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å, —á—Ç–æ –∏ –≤ main.py –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
GPT_MODEL_FOR_TRANSLATION = "gpt-4o-mini"


def _translate_quotes_with_gpt(quotes: list[str]) -> list[str]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ —Ü–∏—Ç–∞—Ç –≤ GPT –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ "–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–≥–æ" –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç.
    """
    if not quotes:
        return []

    # –°–æ–∑–¥–∞–µ–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ü–∏—Ç–∞—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    numbered_quotes = "\n".join([f'{i+1}. "{quote}"' for i, quote in enumerate(quotes)])

    prompt = f"""
–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞. –¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω —Å–ø–∏—Å–æ–∫ —Å—ã—Ä—ã—Ö —Ü–∏—Ç–∞—Ç –∏ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, "–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π" –ø–µ—Ä–µ–≤–æ–¥-–∞–¥–∞–ø—Ç–∞—Ü–∏—é —ç—Ç–∏—Ö —Ü–∏—Ç–∞—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.

–ö–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:
1.  **–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏ –ø–ª–∞–≤–Ω–æ—Å—Ç—å:** –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–ª–∂–µ–Ω –∑–≤—É—á–∞—Ç—å –∫–∞–∫ –∂–∏–≤–∞—è, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä—É—Å—Å–∫–∞—è —Ä–µ—á—å, –∞ –Ω–µ –∫–∞–∫ –¥–æ—Å–ª–æ–≤–Ω—ã–π –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –°–º–µ–ª–æ –∞–¥–∞–ø—Ç–∏—Ä—É–π —Ñ—Ä–∞–∑—ã –∏ –æ–±–æ—Ä–æ—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Å—Ö–æ–¥–Ω—ã–π —Å–º—ã—Å–ª –∏ —Ç–æ–Ω.
2.  **–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –£—á–∏—Ç—ã–≤–∞–π, —á—Ç–æ —ç—Ç–æ –º–Ω–µ–Ω–∏—è –ª–∏–¥–µ—Ä–æ–≤ –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç. –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω–æ.
3.  **–¢–æ—á–Ω–æ—Å—Ç—å:** –ù–µ —Ç–µ—Ä—è–π –∫–ª—é—á–µ–≤—ã–µ –¥–µ—Ç–∞–ª–∏, —Ü–∏—Ñ—Ä—ã –∏ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Å—ã–ª –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è.
4.  **–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:** –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –≤ –≤–∏–¥–µ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞. –ü–æ—Ä—è–¥–æ–∫ –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—É. –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ: –Ω–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –Ω–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –Ω–∏ —Å–≤–æ–∏—Ö –º—ã—Å–ª–µ–π.

–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞:
---
{numbered_quotes}
---

–¢–≤–æ–π –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (—Å—Ç—Ä–æ–≥–æ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫):
"""

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –∫–ª—é—á OpenAI, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ main.py
        if not openai.api_key:
            log("CRITICAL: OpenAI API key is not set. Cannot perform translation.")
            return quotes # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—ã—Ä—ã–µ —Ü–∏—Ç–∞—Ç—ã, –µ—Å–ª–∏ –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω

        log(f"INFO: –û—Ç–ø—Ä–∞–≤–∫–∞ {len(quotes)} —Ü–∏—Ç–∞—Ç –≤ GPT –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞...")
        response = openai.ChatCompletion.create(
            model=GPT_MODEL_FOR_TRANSLATION,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,  # –ù–µ–º–Ω–æ–≥–æ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ç–∏–ª—è
            max_tokens=2048   # –ó–∞–ø–∞—Å —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
        )
        translated_text = response.choices[0].message.content.strip()

        # –ü–∞—Ä—Å–∏–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ –æ—Ç–≤–µ—Ç–∞ GPT
        translated_quotes_list = re.findall(r"^\d+\.\s*\"?(.*?)\"?$", translated_text, re.MULTILINE)

        if len(translated_quotes_list) == len(quotes):
            log("INFO: GPT —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–≤–µ–ª –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª –≤—Å–µ —Ü–∏—Ç–∞—Ç—ã.")
            return translated_quotes_list
        else:
            log(f"ERROR: GPT translation parsing failed. Expected {len(quotes)} quotes, got {len(translated_quotes_list)}. Returning raw quotes.")
            return quotes  # –í–æ–∑–≤—Ä–∞—Ç –∫ —Å—ã—Ä—ã–º –¥–∞–Ω–Ω—ã–º –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞

    except Exception as e:
        log(f"CRITICAL: OpenAI API call failed during translation: {e}")
        return quotes  # –í–æ–∑–≤—Ä–∞—Ç –∫ —Å—ã—Ä—ã–º –¥–∞–Ω–Ω—ã–º –ø—Ä–∏ –æ—à–∏–±–∫–µ API

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Ñ–∞–π–ª–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ª–æ–≥–∏–∫–µ —Å–±–æ—Ä–∞, —Ç–æ–ª—å–∫–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


# 1. –î–∞–Ω–Ω—ã–µ –∏ –∫–ª—é—á–∏
NEWSAPI_KEY     = os.getenv("NEWSAPI_KEY")
YOUTUBE_KEY     = os.getenv("YOUTUBE_KEY")
MASTODON_TOKEN  = os.getenv("MASTODON_TOKEN")
MASTODON_HOST   = os.getenv("MASTODON_HOST", "mastodon.social")
USER_AGENT = "MomentumPulse/1.0 (+https://t.me/MomentumPulse)"
INFLUENCERS = [
    # category: crypto | stock
    {"name": "Elon Musk",           "aliases": ["Elon Musk", "Musk"],           "category": "stock"},
    {"name": "Donald Trump",        "aliases": ["Donald Trump", "Trump"],       "category": "stock"},
    {"name": "Mark Zuckerberg",     "aliases": ["Mark Zuckerberg"],             "category": "stock"},
    {"name": "Jeff Bezos",          "aliases": ["Jeff Bezos"],                  "category": "stock"},
    {"name": "Bill Gates",          "aliases": ["Bill Gates"],                  "category": "stock"},
    {"name": "Warren Buffett",      "aliases": ["Warren Buffett", "Buffett"],   "category": "stock"},
    {"name": "Larry Fink",          "aliases": ["Larry Fink"],                  "category": "stock"},
    {"name": "Ross Ulbricht",       "aliases": ["Ross Ulbricht"],               "category": "crypto"},
    {"name": "Vitalik Buterin",     "aliases": ["Vitalik Buterin", "Buterin"],  "category": "crypto"},
    {"name": "Changpeng Zhao",      "aliases": ["Changpeng Zhao", "CZ"],        "category": "crypto"},
    {"name": "Michael Saylor",      "aliases": ["Michael Saylor"],              "category": "crypto"},
    {"name": "Anthony Pompliano",   "aliases": ["Anthony Pompliano"],           "category": "crypto"},
    {"name": "Balaji Srinivasan",   "aliases": ["Balaji Srinivasan", "Balaji"], "category": "crypto"},
]
LOOKBACK_HOURS = 24
MAX_QUOTES_PER_PERSON = 1
TIMEOUT = 12

# 2. –•–µ–ª–ø–µ—Ä—ã
def _clean_snippet(text: str, max_chars: int = 220) -> str:
    text = html.unescape(text).strip().replace("\n", " ")
    text = re.sub(r"\s{2,}", " ", text)
    sentences = re.split(r"(?<=[.!?])\s+", text)
    snippet = ""
    for sent in sentences:
        if len(snippet) + len(sent) <= max_chars:
            snippet = f"{snippet} {sent}".strip()
        else:
            break
    if not snippet:
        snippet = text[: max_chars].rsplit(" ", 1)[0] + "‚Ä¶"
    return snippet
_cut = _clean_snippet

# 3. –§—É–Ω–∫—Ü–∏–∏ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def _fetch_reddit(alias: str) -> list[str]:
    url = (
        f"https://www.reddit.com/search.json?q=\"{requests.utils.quote(alias)}\""
        f"&sort=new&limit=10&restrict_sr=0&syntax=plain"
    )
    try:
        r = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=TIMEOUT)
        r.raise_for_status()
        posts = r.json().get("data", {}).get("children", [])
        out = []
        ts_limit = int((datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)).timestamp())
        for p in posts:
            data = p.get("data", {})
            if data.get("created_utc", 0) < ts_limit:
                continue
            body = data.get("selftext") or data.get("title", "")
            if body:
                out.append(_cut(body))
            if len(out) >= MAX_QUOTES_PER_PERSON:
                break
        return out
    except Exception as e:
        log(f"Reddit error ({alias}): {e}")
        return []

def _fetch_newsapi(alias: str) -> list[str]:
    if not NEWSAPI_KEY: return []
    url = "https://newsapi.org/v2/everything"
    params = {"qInTitle": alias, "sortBy": "publishedAt", "language": "en", "pageSize": 5, "apiKey": NEWSAPI_KEY}
    try:
        r = requests.get(url, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        news = r.json().get("articles", [])
        out = []
        ts_limit = datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)
        for n in news:
            published = n.get("publishedAt", "")[:19]
            try:
                if datetime.fromisoformat(published.replace("Z", "")) < ts_limit:
                    continue
            except: pass
            out.append(_cut(n.get("title", "")))
            if len(out) >= MAX_QUOTES_PER_PERSON: break
        return out
    except Exception as e:
        log(f"NewsAPI error ({alias}): {e}")
        return []

def _fetch_youtube(alias: str) -> list[str]:
    if not YOUTUBE_KEY: return []
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {"part": "snippet", "q": alias, "maxResults": 5, "order": "date", "type": "video", "key": YOUTUBE_KEY}
    try:
        r = requests.get(url, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        items = r.json().get("items", [])
        out = []
        ts_limit = (datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS))
        for it in items:
            sn = it.get("snippet", {})
            published = sn.get("publishedAt", "")[:19]
            try:
                if datetime.fromisoformat(published.replace("Z", "")) < ts_limit:
                    continue
            except: pass
            title = sn.get("title", "")
            if title:
                out.append(_cut(title))
            if len(out) >= MAX_QUOTES_PER_PERSON: break
        return out
    except Exception as e:
        log(f"YouTube error ({alias}): {e}")
        return []

def _fetch_mastodon(alias: str) -> list[str]:
    if not MASTODON_TOKEN: return []
    url = f"https://{MASTODON_HOST}/api/v2/search"
    params = {"q": alias, "limit": 5, "resolve": "true"}
    headers = {"Authorization": f"Bearer {MASTODON_TOKEN}", "User-Agent": USER_AGENT}
    try:
        r = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        statuses = r.json().get("statuses", [])
        out = []
        ts_limit = (datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)).timestamp()
        for st in statuses:
            created = datetime.fromisoformat(st["created_at"][:-1])
            if created.timestamp() < ts_limit: continue
            text = re.sub("<.*?>", "", st["content"])
            out.append(_cut(text))
            if len(out) >= MAX_QUOTES_PER_PERSON: break
        return out
    except Exception as e:
        log(f"Mastodon error ({alias}): {e}")
        return []

# 4. –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏ –ø–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
SRC_FUNCS = [_fetch_reddit, _fetch_newsapi, _fetch_youtube, _fetch_mastodon]
def _collect_for_aliases(aliases: list[str]) -> list[str]:
    quotes = []
    for alias in aliases:
        for fn in SRC_FUNCS:
            quotes.extend(fn(alias))
            if len(quotes) >= MAX_QUOTES_PER_PERSON: break
        if len(quotes) >= MAX_QUOTES_PER_PERSON: break
    return [_clean_snippet(q) for q in quotes[:MAX_QUOTES_PER_PERSON]]

def _build_block(category: str) -> str:
    influencers_with_quotes = []
    raw_quotes_to_translate = []

    # –®–∞–≥ 1: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ "—Å—ã—Ä—ã–µ" —Ü–∏—Ç–∞—Ç—ã –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫
    for inf in INFLUENCERS:
        if inf["category"] != category:
            continue
        q = _collect_for_aliases(inf["aliases"])
        if q:
            influencers_with_quotes.append(inf)
            raw_quotes_to_translate.append(q[0])

    if not raw_quotes_to_translate:
        return ""

    # –®–∞–≥ 2: –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤—Å–µ —Ü–∏—Ç–∞—Ç—ã –æ–¥–Ω–∏–º –ø–∞–∫–µ—Ç–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º –∫ GPT
    translated_quotes = _translate_quotes_with_gpt(raw_quotes_to_translate)

    # –®–∞–≥ 3: –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –±–ª–æ–∫ —Å –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–º–∏ —Ü–∏—Ç–∞—Ç–∞–º–∏
    bullets = []
    if len(translated_quotes) == len(influencers_with_quotes):
        for inf, translated_q in zip(influencers_with_quotes, translated_quotes):
            bullets.append(f"‚Äî <b>{inf['name']}</b>: {translated_q}")
    else:
        # –ü–ª–∞–Ω –ë: –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –≤—ã–≤–æ–¥–∏–º —á—Ç–æ –µ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—ã—Ä—ã–µ —Ü–∏—Ç–∞—Ç—ã)
        log("ERROR: Mismatch in translated quotes count. Falling back to raw quotes.")
        for inf, raw_q in zip(influencers_with_quotes, raw_quotes_to_translate):
            bullets.append(f"‚Äî <b>{inf['name']}</b>: {raw_q} (–æ—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞)")

    if not bullets:
        return ""

    title = "üó£Ô∏è –ú–Ω–µ–Ω–∏—è –∫—Ä–∏–ø—Ç–æ-–ª–∏–¥–µ—Ä–æ–≤" if category == "crypto" \
            else "üó£Ô∏è –í—ã–¥–µ—Ä–∂–∫–∏ –æ—Ç –ª—é–¥–µ–π, –≤–ª–∏—è—é—â–∏—Ö –Ω–∞ —Ñ–æ–Ω–¥–æ–≤—ã–π —Ä—ã–Ω–æ–∫"
    return "\n".join([title] + bullets)

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def get_crypto_quotes_block() -> str:
    return _build_block("crypto")

def get_stock_quotes_block() -> str:
    return _build_block("stock")