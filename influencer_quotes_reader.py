# influencer_quotes_reader.py
# v3.0 ‚Äì 10-Jun-2025
#
# GPT —Ç–µ–ø–µ—Ä—å –≤—ã—Å—Ç—É–ø–∞–µ—Ç –≤ —Ä–æ–ª–∏ —É–º–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞:
# 1. –§–∏–ª—å—Ç—Ä—É–µ—Ç –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã.
# 2. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º–∞—Ç–∏–∫—É –∫–∞–∂–¥–æ–π —Ü–∏—Ç–∞—Ç—ã –ø–æ –µ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.
# 3. –î–µ–ª–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥.

import os
import time
import html
import re
import requests
import openai
import json
from datetime import datetime, timedelta
from custom_logger import log

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GPT ---
GPT_MODEL_FOR_PROCESSING = "gpt-4o-mini"

def _process_quotes_with_gpt(raw_quotes: list[str]) -> list:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç "—Å—ã—Ä—ã–µ" —Ü–∏—Ç–∞—Ç—ã –≤ GPT –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç.
    """
    if not raw_quotes:
        return []

    # –°–æ–∑–¥–∞–µ–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    numbered_quotes_str = "\n".join([f'{i+1}. "{quote}"' for i, quote in enumerate(raw_quotes)])

    prompt = f"""
–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∏ —É–º–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞. –¢–µ–±–µ –¥–∞–Ω —Å–ø–∏—Å–æ–∫ "—Å—ã—Ä—ã—Ö" —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç:

–≠—Ç–∞–ø 1: –û–¶–ï–ù–ö–ê. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç. –Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º, —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ–º –∏–ª–∏ –º–Ω–µ–Ω–∏–µ–º?
–û—Ç–±—Ä–∞—Å—ã–≤–∞–π (–∏–≥–Ω–æ—Ä–∏—Ä—É–π) —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ —è–≤–ª—è—é—Ç—Å—è:
- –ü—Ä–æ—Å—Ç–æ –Ω–∞–±–æ—Ä–æ–º —Ö—ç—à—Ç–µ–≥–æ–≤.
- –ó–∞–≥–æ–ª–æ–≤–∫–æ–º —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ –≤–∏–¥–µ–æ, –∞ –Ω–µ —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ –Ω–µ–≥–æ.
- –ù–æ–≤–æ—Å—Ç—å—é –û —á–µ–ª–æ–≤–µ–∫–µ, –∞ –Ω–µ –ï–ì–û –º–Ω–µ–Ω–∏–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –æ–±—Å—É–∂–¥–∞—é—Ç —Å–ª–æ–≤–∞ –¢—Ä–∞–º–ø–∞" ‚Äî —ç—Ç–æ –º—É—Å–æ—Ä).
- –ë–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º –æ–±—Ä—ã–≤–∫–æ–º —Ñ—Ä–∞–∑—ã –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
- –ü—Ä–æ—Å—Ç–æ —Å—Å—ã–ª–∫–æ–π (URL).

–≠—Ç–∞–ø 2: –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–ò–Ø. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞, –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –æ—Ü–µ–Ω–∫—É, –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ –≥–ª–∞–≤–Ω—É—é —Ç–µ–º—É –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é. –¢–µ–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ 'crypto' –∏–ª–∏ 'stock'.
- 'crypto': –µ—Å–ª–∏ —Ä–µ—á—å –æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞—Ö, –±–ª–æ–∫—á–µ–π–Ω–µ, NFT, —Ç–æ–∫–µ–Ω–∞—Ö (BTC, ETH –∏ —Ç.–¥.).
- 'stock': –µ—Å–ª–∏ —Ä–µ—á—å –æ —Ñ–æ–Ω–¥–æ–≤–æ–º —Ä—ã–Ω–∫–µ, –∞–∫—Ü–∏—è—Ö, —ç–∫–æ–Ω–æ–º–∏–∫–µ, —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏—è—Ö.

–≠—Ç–∞–ø 3: –ü–ï–†–ï–í–û–î. –í—ã–ø–æ–ª–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, "–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π" –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞. –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º, –∫–∞–∫ –±—É–¥—Ç–æ –µ–≥–æ –Ω–∞–ø–∏—Å–∞–ª –Ω–æ—Å–∏—Ç–µ–ª—å —è–∑—ã–∫–∞.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –≤–∏–¥–µ JSON-–º–∞—Å—Å–∏–≤–∞. –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –º–∞—Å—Å–∏–≤–∞ ‚Äî —ç—Ç–æ –æ–±—ä–µ–∫—Ç –¥–ª—è –û–î–ù–û–ô –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç—ã.
–ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç—Ä–∏ –∫–ª—é—á–∞:
- "original_index": –Ω–æ–º–µ—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ (–Ω–∞—á–∏–Ω–∞—è —Å 1).
- "theme": –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–æ–±–æ–π —Ç–µ–º–∞ ('crypto' –∏–ª–∏ 'stock').
- "translated_quote": —Ç–≤–æ–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥.

–ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–µ –ø—Ä–æ—à–µ–ª —Ç–≤–æ—é –æ—Ü–µ–Ω–∫—É, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ [].

"–°—ã—Ä—ã–µ" —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:
---
{numbered_quotes_str}
---

–¢–≤–æ–π JSON-–æ—Ç–≤–µ—Ç:
"""
    try:
        if not openai.api_key:
            log("CRITICAL: OpenAI API key not set. Cannot process quotes.")
            return []

        log(f"INFO: –û—Ç–ø—Ä–∞–≤–∫–∞ {len(raw_quotes)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ GPT –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞...")
        response = openai.ChatCompletion.create(
            model=GPT_MODEL_FOR_PROCESSING,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=3000,
            response_format={"type": "json_object"}
        )
        response_text = response.choices[0].message.content.strip()
        
        # GPT-4o —Å `json_object` –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å JSON –∫–∞–∫ —Å—Ç—Ä–æ–∫—É –≤–Ω—É—Ç—Ä–∏ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
        # –ù–∞–ø—Ä–∏–º–µ—Ä: {"quotes": [...]}. –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –º–∞—Å—Å–∏–≤.
        parsed_json = json.loads(response_text)
        processed_quotes = next(iter(parsed_json.values())) # –ë–µ—Ä–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∫–ª—é—á–∞

        if isinstance(processed_quotes, list):
            log(f"INFO: GPT –æ–±—Ä–∞–±–æ—Ç–∞–ª –∏ –≤–µ—Ä–Ω—É–ª {len(processed_quotes)} –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç.")
            return processed_quotes
        else:
            log("ERROR: GPT returned a non-list object. Fallback to empty.")
            return []

    except Exception as e:
        log(f"CRITICAL: GPT call or JSON parsing failed during quote processing: {e}")
        return []

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –§—É–Ω–∫—Ü–∏–∏ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# –î–∞–Ω–Ω—ã–µ –∏ –∫–ª—é—á–∏
NEWSAPI_KEY     = os.getenv("NEWSAPI_KEY")
YOUTUBE_KEY     = os.getenv("YOUTUBE_KEY")
MASTODON_TOKEN  = os.getenv("MASTODON_TOKEN")
MASTODON_HOST   = os.getenv("MASTODON_HOST", "mastodon.social")
USER_AGENT = "MomentumPulse/1.0 (+https://t.me/MomentumPulse)"
INFLUENCERS = [
    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ —Å–±–æ—Ä–∞, GPT –ø—Ä–∏–º–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
    {"name": "Elon Musk",           "aliases": ["Elon Musk", "Musk"],           "category": "stock"},
    {"name": "Donald Trump",        "aliases": ["Donald Trump", "Trump"],       "category": "stock"},
    {"name": "Mark Zuckerberg",     "aliases": ["Mark Zuckerberg"],             "category": "stock"},
    {"name": "Jeff Bezos",          "aliases": ["Jeff Bezos"],                  "category": "stock"},
    {"name": "Bill Gates",          "aliases": ["Bill Gates"],                  "category": "stock"},
    {"name": "Warren Buffett",      "aliases": ["Warren Buffett", "Buffett"],   "category": "stock"},
    {"name": "Larry Fink",          "aliases": ["Larry Fink"],                  "category": "stock"},
    {"name": "Ross Ulbricht",       "aliases": ["Ross Ulbricht"],               "category": "crypto"},
    {"name": "Vitalik Buterin",     "aliases": ["Vitalik Buterin", "Buterin"],  "category": "crypto"},
    {"name": "Changpeng Zhao",      "aliases": ["Changpeng Zhao", "CZ"],        "category": "crypto"},
    {"name": "Michael Saylor",      "aliases": ["Michael Saylor"],              "category": "crypto"},
    {"name": "Anthony Pompliano",   "aliases": ["Anthony Pompliano"],           "category": "crypto"},
    {"name": "Balaji Srinivasan",   "aliases": ["Balaji Srinivasan", "Balaji"], "category": "crypto"},
]
LOOKBACK_HOURS = 24
MAX_QUOTES_PER_PERSON = 1
TIMEOUT = 12

def _clean_snippet(text: str, max_chars: int = 220) -> str:
    text = html.unescape(text).strip().replace("\n", " ")
    text = re.sub(r"\s{2,}", " ", text)
    sentences = re.split(r"(?<=[.!?])\s+", text)
    snippet = ""
    for sent in sentences:
        if len(snippet) + len(sent) <= max_chars:
            snippet = f"{snippet} {sent}".strip()
        else:
            break
    if not snippet:
        snippet = text[: max_chars].rsplit(" ", 1)[0] + "‚Ä¶"
    return snippet

# –§—É–Ω–∫—Ü–∏–∏ _fetch_* –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

def _fetch_reddit(alias: str) -> list[str]:
    url = (f"https://www.reddit.com/search.json?q=\"{requests.utils.quote(alias)}\"&sort=new&limit=10&restrict_sr=0&syntax=plain")
    try:
        r = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=TIMEOUT)
        r.raise_for_status()
        posts = r.json().get("data", {}).get("children", [])
        out = []
        ts_limit = int((datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)).timestamp())
        for p in posts:
            data = p.get("data", {})
            if data.get("created_utc", 0) < ts_limit: continue
            body = data.get("selftext") or data.get("title", "")
            if body: out.append(_clean_snippet(body))
            if len(out) >= MAX_QUOTES_PER_PERSON: break
        return out
    except Exception as e:
        log(f"Reddit error ({alias}): {e}")
        return []

def _fetch_newsapi(alias: str) -> list[str]:
    if not NEWSAPI_KEY: return []
    url = "https://newsapi.org/v2/everything"
    params = {"qInTitle": alias, "sortBy": "publishedAt", "language": "en", "pageSize": 5, "apiKey": NEWSAPI_KEY}
    try:
        r = requests.get(url, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        news = r.json().get("articles", [])
        out = []
        ts_limit = datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)
        for n in news:
            published = n.get("publishedAt", "")[:19]
            try:
                if datetime.fromisoformat(published.replace("Z", "")) < ts_limit: continue
            except: pass
            out.append(_clean_snippet(n.get("title", "")))
            if len(out) >= MAX_QUOTES_PER_PERSON: break
        return out
    except Exception as e:
        log(f"NewsAPI error ({alias}): {e}")
        return []

def _fetch_youtube(alias: str) -> list[str]:
    if not YOUTUBE_KEY: return []
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {"part": "snippet", "q": alias, "maxResults": 5, "order": "date", "type": "video", "key": YOUTUBE_KEY}
    try:
        r = requests.get(url, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        items = r.json().get("items", [])
        out = []
        ts_limit = (datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS))
        for it in items:
            sn = it.get("snippet", {})
            published = sn.get("publishedAt", "")[:19]
            try:
                if datetime.fromisoformat(published.replace("Z", "")) < ts_limit: continue
            except: pass
            title = sn.get("title", "")
            if title: out.append(_clean_snippet(title))
            if len(out) >= MAX_QUOTES_PER_PERSON: break
        return out
    except Exception as e:
        log(f"YouTube error ({alias}): {e}")
        return []

def _fetch_mastodon(alias: str) -> list[str]:
    if not MASTODON_TOKEN: return []
    url = f"https://{MASTODON_HOST}/api/v2/search"
    params = {"q": alias, "limit": 5, "resolve": "true"}
    headers = {"Authorization": f"Bearer {MASTODON_TOKEN}", "User-Agent": USER_AGENT}
    try:
        r = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        statuses = r.json().get("statuses", [])
        out = []
        ts_limit = (datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)).timestamp()
        for st in statuses:
            created = datetime.fromisoformat(st["created_at"][:-1])
            if created.timestamp() < ts_limit: continue
            text = re.sub("<.*?>", "", st["content"])
            out.append(_clean_snippet(text))
            if len(out) >= MAX_QUOTES_PER_PERSON: break
        return out
    except Exception as e:
        log(f"Mastodon error ({alias}): {e}")
        return []


SRC_FUNCS = [_fetch_reddit, _fetch_newsapi, _fetch_youtube, _fetch_mastodon]
def _collect_for_aliases(aliases: list[str]) -> list[str]:
    quotes = []
    for alias in aliases:
        for fn in SRC_FUNCS:
            quotes.extend(fn(alias))
            if len(quotes) >= MAX_QUOTES_PER_PERSON: break
        if len(quotes) >= MAX_QUOTES_PER_PERSON: break
    return [_clean_snippet(q) for q in quotes[:MAX_QUOTES_PER_PERSON]]

# --- –ù–æ–≤–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º–∞—è —Ñ—É–Ω–∫—Ü–∏—è ---
def get_all_influencer_quotes() -> dict:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ü–∏—Ç–∞—Ç—ã, –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ GPT –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç
    –¥–≤–∞ –≥–æ—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–∞: –¥–ª—è –∫—Ä–∏–ø—Ç–æ –∏ —Ñ–æ–Ω–¥—ã.
    """
    influencers_with_quotes = []
    raw_quotes = []

    # –®–∞–≥ 1: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ "—Å—ã—Ä—ã–µ" —Ü–∏—Ç–∞—Ç—ã –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫
    for inf in INFLUENCERS:
        q = _collect_for_aliases(inf["aliases"])
        if q:
            influencers_with_quotes.append(inf)
            raw_quotes.append(q[0])

    if not raw_quotes:
        return {"crypto": "", "stock": ""}

    # –®–∞–≥ 2: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å—ë —á–µ—Ä–µ–∑ GPT
    processed_quotes = _process_quotes_with_gpt(raw_quotes)

    # –®–∞–≥ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    crypto_bullets = []
    stock_bullets = []

    for quote_data in processed_quotes:
        original_index = quote_data.get("original_index")
        theme = quote_data.get("theme")
        translated_quote = quote_data.get("translated_quote")

        # –ò–Ω–¥–µ–∫—Å –≤ JSON –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 1, –≤ Python-—Å–ø–∏—Å–∫–µ —Å 0
        if original_index is not None and 1 <= original_index <= len(influencers_with_quotes):
            influencer_name = influencers_with_quotes[original_index - 1]["name"]
            bullet = f"‚Äî <b>{influencer_name}</b>: {translated_quote}"
            
            if theme == 'crypto':
                crypto_bullets.append(bullet)
            elif theme == 'stock':
                stock_bullets.append(bullet)

    # –®–∞–≥ 4: –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏
    crypto_block = ""
    if crypto_bullets:
        title = "üó£Ô∏è –ú–Ω–µ–Ω–∏—è –∫—Ä–∏–ø—Ç–æ-–ª–∏–¥–µ—Ä–æ–≤"
        crypto_block = "\n".join([title] + crypto_bullets)

    stock_block = ""
    if stock_bullets:
        title = "üó£Ô∏è –í—ã–¥–µ—Ä–∂–∫–∏ –æ—Ç –ª—é–¥–µ–π, –≤–ª–∏—è—é—â–∏—Ö –Ω–∞ —Ñ–æ–Ω–¥–æ–≤—ã–π —Ä—ã–Ω–æ–∫"
        stock_block = "\n".join([title] + stock_bullets)
        
    return {"crypto": crypto_block, "stock": stock_block}