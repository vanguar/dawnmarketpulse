# influencer_quotes_reader.py
# v1.0 ‚Äì 09-Jun-2025
#
# –ó–∞–±–∏—Ä–∞–µ–º —Å–≤–µ–∂–∏–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è –∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä–æ–≤ –∏–∑ Reddit, NewsAPI, YouTube –∏ Mastodon
# –±–µ–∑ –ø–ª–∞—Ç–Ω–æ–≥–æ Twitter API.  –í—Å—ë ¬´read-only¬ª, –±–µ–∑ Selenium.

import os, time, html, re, requests
from datetime import datetime, timedelta
from custom_logger import log     # —É–∂–µ –µ—Å—Ç—å –≤ –ø—Ä–æ–µ–∫—Ç–µ
from googletrans import Translator
translator = Translator()

def _ru(text: str) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ü–∏—Ç–∞—Ç—É –Ω–∞ —Ä—É—Å—Å–∫–∏–π, –µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω–∏–∫ –Ω–µ ru/uk."""
    try:
        tr = translator.translate(text, dest="ru")
        return tr.text
    except Exception as e:
        log(f"Translate error: {e}")
        return text


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1. –î–∞–Ω–Ω—ã–µ –∏ –∫–ª—é—á–∏

NEWSAPI_KEY     = os.getenv("NEWSAPI_KEY")     # https://newsapi.org
YOUTUBE_KEY     = os.getenv("YOUTUBE_KEY")     # console.cloud.google.com
MASTODON_TOKEN  = os.getenv("MASTODON_TOKEN")  # –ª—é–±–æ–π –ø—É–±–ª–∏—á–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å
MASTODON_HOST   = os.getenv("MASTODON_HOST", "mastodon.social")

USER_AGENT = "MomentumPulse/1.0 (+https://t.me/MomentumPulse)"

INFLUENCERS = [
    # category: crypto | stock
    {"name": "Elon Musk",           "aliases": ["Elon Musk", "Musk"],           "category": "stock"},
    {"name": "Donald Trump",        "aliases": ["Donald Trump", "Trump"],       "category": "stock"},
    {"name": "Mark Zuckerberg",     "aliases": ["Mark Zuckerberg"],             "category": "stock"},
    {"name": "Jeff Bezos",          "aliases": ["Jeff Bezos"],                  "category": "stock"},
    {"name": "Bill Gates",          "aliases": ["Bill Gates"],                  "category": "stock"},
    {"name": "Warren Buffett",      "aliases": ["Warren Buffett", "Buffett"],   "category": "stock"},
    {"name": "Larry Fink",          "aliases": ["Larry Fink"],                  "category": "stock"},
    {"name": "Ross Ulbricht",       "aliases": ["Ross Ulbricht"],               "category": "crypto"},
    {"name": "Vitalik Buterin",     "aliases": ["Vitalik Buterin", "Buterin"],  "category": "crypto"},
    {"name": "Changpeng Zhao",      "aliases": ["Changpeng Zhao", "CZ"],        "category": "crypto"},
    {"name": "Michael Saylor",      "aliases": ["Michael Saylor"],              "category": "crypto"},
    {"name": "Anthony Pompliano",   "aliases": ["Anthony Pompliano"],           "category": "crypto"},
    {"name": "Balaji Srinivasan",   "aliases": ["Balaji Srinivasan", "Balaji"], "category": "crypto"},
]

LOOKBACK_HOURS = 24          # –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏
MAX_QUOTES_PER_PERSON = 1    # –ª–∏—à–Ω–µ–µ –Ω–µ –Ω—É–∂–Ω–æ ‚Äì –æ—Ç—á—ë—Ç –∏ —Ç–∞–∫ –¥–ª–∏–Ω–Ω—ã–π
TIMEOUT = 12


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. –•–µ–ª–ø–µ—Ä—ã

def _clean_snippet(text: str, max_chars: int = 220) -> str:
    """
    –û—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤–æ–µ(-—ã–µ) –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º, —á—Ç–æ–±—ã —É–º–µ—â–∞–ª–æ—Å—å –≤ max_chars.
    –ï—Å–ª–∏ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—é—â–µ–µ ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ä–µ–∂–µ–º –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –ø—Ä–æ–±–µ–ª—É.
    """
    text = html.unescape(text).strip().replace("\n", " ")
    # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"\s{2,}", " ", text)

    # —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ .!?  (–Ω–æ –Ω–µ –ø–æ   ‚Ä¶)
    sentences = re.split(r"(?<=[.!?])\s+", text)
    snippet = ""
    for sent in sentences:
        if len(snippet) + len(sent) <= max_chars:
            snippet = f"{snippet} {sent}".strip()
        else:
            break

    if not snippet:                   # –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–∞–º–æ –ø–æ —Å–µ–±–µ –∑–¥–æ—Ä–æ–≤–µ–Ω–Ω–æ–µ
        snippet = text[: max_chars].rsplit(" ", 1)[0] + "‚Ä¶"
    return snippet

# ‚îÄ‚îÄ alias for old calls ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_cut = _clean_snippet


def _to_ts(dt: datetime) -> int:
    return int(dt.replace(tzinfo=None).timestamp())

def _since_param(hours_back: int = LOOKBACK_HOURS):
    t = datetime.utcnow() - timedelta(hours=hours_back)
    return _to_ts(t)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3.   Reddit  (–±–µ–∑ auth ‚Äì –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞)

def _fetch_reddit(alias: str) -> list[str]:
    url = (
        f"https://www.reddit.com/search.json?q=\"{requests.utils.quote(alias)}\""
        f"&sort=new&limit=10&restrict_sr=0&syntax=plain"
    )
    try:
        r = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=TIMEOUT)
        r.raise_for_status()
        posts = r.json().get("data", {}).get("children", [])
        out = []
        ts_limit = _since_param()
        for p in posts:
            data = p.get("data", {})
            if data.get("created_utc", 0) < ts_limit:
                continue
            body = data.get("selftext") or data.get("title", "")
            if body:
                out.append(_cut(body))
            if len(out) >= MAX_QUOTES_PER_PERSON:
                break
        return out
    except Exception as e:
        log(f"Reddit error ({alias}): {e}")
        return []


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4.   NewsAPI

def _fetch_newsapi(alias: str) -> list[str]:
    if not NEWSAPI_KEY:
        return []
    url = "https://newsapi.org/v2/everything"
    params = {
        "qInTitle": alias,
        "sortBy": "publishedAt",
        "language": "en",
        "pageSize": 5,
        "apiKey": NEWSAPI_KEY,
    }
    try:
        r = requests.get(url, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        news = r.json().get("articles", [])
        out = []
        ts_limit = datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)
        for n in news:
            published = n.get("publishedAt", "")[:19]  # 2025-06-09T08:05:00Z
            try:
                if datetime.fromisoformat(published.replace("Z", "")) < ts_limit:
                    continue
            except:  # –ø–∞—Ä—Å–µ—Ä—É –Ω–∞—Å—Ä–∞—Ç—å
                pass
            out.append(_cut(n.get("title", "")))
            if len(out) >= MAX_QUOTES_PER_PERSON:
                break
        return out
    except Exception as e:
        log(f"NewsAPI error ({alias}): {e}")
        return []


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5.   YouTube (title + description –Ω–æ–≤—ã—Ö –≤–∏–¥–µ–æ)

def _fetch_youtube(alias: str) -> list[str]:
    if not YOUTUBE_KEY:
        return []
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "q": alias,
        "maxResults": 5,
        "order": "date",
        "type": "video",
        "key": YOUTUBE_KEY,
    }
    try:
        r = requests.get(url, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        items = r.json().get("items", [])
        out = []
        ts_limit = _since_param()
        for it in items:
            sn = it.get("snippet", {})
            published = sn.get("publishedAt", "")[:19]
            try:
                if datetime.fromisoformat(published.replace("Z", "")) < datetime.utcfromtimestamp(ts_limit):
                    continue
            except:
                pass
            title = sn.get("title", "")
            if title:
                out.append(_cut(title))
            if len(out) >= MAX_QUOTES_PER_PERSON:
                break
        return out
    except Exception as e:
        log(f"YouTube error ({alias}): {e}")
        return []


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 6.   Mastodon  (–ø–æ–∏—Å–∫ –ø–æ –ø—É–±–ª–∏—á–Ω–æ–º—É /api/v2/search)

def _fetch_mastodon(alias: str) -> list[str]:
    if not MASTODON_TOKEN:
        return []
    url = f"https://{MASTODON_HOST}/api/v2/search"
    params = {"q": alias, "limit": 5, "resolve": "true"}
    headers = {"Authorization": f"Bearer {MASTODON_TOKEN}", "User-Agent": USER_AGENT}
    try:
        r = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        statuses = r.json().get("statuses", [])
        out = []
        ts_limit = _since_param()
        for st in statuses:
            created = datetime.fromisoformat(st["created_at"][:-1])
            if created.timestamp() < ts_limit:
                continue
            # —É–±–∏—Ä–∞–µ–º html-—Ç–µ–≥–∏
            text = re.sub("<.*?>", "", st["content"])
            out.append(_cut(text))
            if len(out) >= MAX_QUOTES_PER_PERSON:
                break
        return out
    except Exception as e:
        log(f"Mastodon error ({alias}): {e}")
        return []


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 7.   –ê–≥—Ä–µ–≥–∞—Ü–∏—è

SRC_FUNCS = [_fetch_reddit, _fetch_newsapi, _fetch_youtube, _fetch_mastodon]

def _collect_for_aliases(aliases: list[str]) -> list[str]:
    quotes = []
    for alias in aliases:
        for fn in SRC_FUNCS:
            quotes.extend(fn(alias))
            if len(quotes) >= MAX_QUOTES_PER_PERSON:
                break
        if len(quotes) >= MAX_QUOTES_PER_PERSON:
            break

    #  –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π —Å–Ω–∏–ø–ø–µ—Ç –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞
    return [_clean_snippet(q) for q in quotes[:MAX_QUOTES_PER_PERSON]]



def _build_block(category: str) -> str:
    bullets = []
    for inf in INFLUENCERS:
        if inf["category"] != category:
            continue
        q = _collect_for_aliases(inf["aliases"])
        if q:
            bullets.append(f"‚Äî <b>{inf['name']}</b>: {q[0]}")
    if not bullets:
        return ""
    title = "üó£Ô∏è –ú–Ω–µ–Ω–∏—è –∫—Ä–∏–ø—Ç–æ-–ª–∏–¥–µ—Ä–æ–≤" if category == "crypto" \
            else "üó£Ô∏è –í—ã–¥–µ—Ä–∂–∫–∏ –æ—Ç –ª—é–¥–µ–π, –≤–ª–∏—è—é—â–∏—Ö –Ω–∞ —Ñ–æ–Ω–¥–æ–≤—ã–π —Ä—ã–Ω–æ–∫"
    return "\n".join([title] + bullets)

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_crypto_quotes_block() -> str:
    return _build_block("crypto")

def get_stock_quotes_block() -> str:
    return _build_block("stock")
