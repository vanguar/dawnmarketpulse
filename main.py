#!/usr/bin/env python3
import nltk
nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)

import os
import sys
import requests
import openai
from datetime import datetime, timezone, date
from time import sleep
import traceback
import re

from market_reader import get_market_data_text, get_crypto_data
from news_reader import get_news_block
from analyzer import keyword_alert, store_and_compare
from report_utils import analyze_sentiment

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
openai.api_key = os.getenv("OPENAI_KEY")
TG_TOKEN = os.getenv("TG_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

MODEL = "gpt-4o-mini"
TIMEOUT = 60  # –¢–∞–π–º–∞—É—Ç –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, OpenAI)
TG_LIMIT_BYTES = 3800  # –ë–∞–π—Ç–æ–≤—ã–π –ª–∏–º–∏—Ç –¥–ª—è –¢–ï–ö–°–¢–ê –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞)
GPT_TOKENS = 400 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT

GPT_CONTINUATION = """–ê–∫—Ü–∏–∏-–ª–∏–¥–µ—Ä—ã üöÄ / –ê—É—Ç—Å–∞–π–¥–µ—Ä—ã üìâ
- –ø–æ 2‚Äì3 –±—É–º–∞–≥–∏ + –ø—Ä–∏—á–∏–Ω–∞
‚Üí –í—ã–≤–æ–¥.

–ú–∞–∫—Ä–æ-–Ω–æ–≤–æ—Å—Ç–∏ üì∞
- 3 –≥–ª–∞–≤–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞ + –≤–ª–∏—è–Ω–∏–µ

–¶–∏—Ç–∞—Ç—ã –¥–Ω—è üó£
- –¥–æ 2 —Ü–∏—Ç–∞—Ç + —Å–º—ã—Å–ª

–ß–∏—Å–ª–æ-—Ñ–∞–∫—Ç ü§î

‚ö°Ô∏è –ò–¥–µ—è –¥–Ω—è ‚Äì 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è actionable-—Å–æ–≤–µ—Ç–∞.

‚ÄºÔ∏è –¢–æ–ª—å–∫–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç, –±–µ–∑ HTML.
‚ÄºÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π —Ç–µ–∫—Å—Ç —Å –î–í–û–ô–ù–´–ú–ò –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫ –º–µ–∂–¥—É –∞–±–∑–∞—Ü–∞–º–∏.
‚ÄºÔ∏è –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —Ä–∞–∑–¥–µ–ª–æ–≤."""

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def log(msg):
    timestamp = f"[{datetime.now(timezone.utc):%Y-%m-%d %H:%M:%S} UTC]"
    print(f"{timestamp} {msg}", flush=True)

def safe_call(func, retries=3, delay=5, label="‚ùó –û—à–∏–±–∫–∞"):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            log(f"{label}: –ø–æ–ø—ã—Ç–∫–∞ {i + 1}/{retries} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            if i < retries - 1:
                sleep(delay)
    log(f"{label}: –≤—Å–µ {retries} –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–µ–Ω—ã.")
    return None

# --- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ ---

def gpt_report():
    today = date.today().strftime("%d.%m.%Y")
    header = f"üìÖ –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ {today}"
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
    market_data_text = get_market_data_text()
    crypto_data_text = get_crypto_data(extended=True)
    news_block_text = get_news_block()

    dynamic_data = (
        f"{header}\n\n"
        f"{market_data_text}\n\n"
        f"{crypto_data_text}\n\n"
        f"{news_block_text}\n\n" # get_news_block –£–ñ–ï —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ GPT_CONTINUATION –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
        f"{GPT_CONTINUATION}" # –≠—Ç–æ –æ–±—â–∏–π GPT_CONTINUATION –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å–µ–∫—Ü–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ –±—É–¥—É—Ç
    )
    
    log(f"‚ÑπÔ∏è –î–∞–Ω–Ω—ã–µ –¥–ª—è GPT (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤): {dynamic_data[:300]}...")
    log(f"‚ÑπÔ∏è –û–±—â–∞—è –¥–ª–∏–Ω–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è GPT: {len(dynamic_data)} —Å–∏–º–≤–æ–ª–æ–≤")

    response = safe_call(
        lambda: openai.ChatCompletion.create(
            model=MODEL,
            messages=[{"role": "user", "content": dynamic_data}],
            timeout=TIMEOUT,
            temperature=0.4,
            max_tokens=GPT_TOKENS,
        ),
        label="‚ùó –û—à–∏–±–∫–∞ OpenAI"
    )
    if not response:
        raise RuntimeError("OpenAI –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")
    
    generated_text = response.choices[0].message.content.strip()
    log(f"üìù GPT —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç ({len(generated_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
    return generated_text

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ Telegram ---

def prepare_text(text):
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å —ç–º–æ–¥–∑–∏ –≤—Å–µ–≥–¥–∞ –¥–≤–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫–∏
    for marker in ["üìä", "üöÄ", "üìâ", "‚Çø", "üì∞", "üó£", "ü§î", "‚ö°Ô∏è", "üîç", "üìà", "üß†"]: # –î–æ–±–∞–≤–∏–ª –º–∞—Ä–∫–µ—Ä—ã –∏–∑ main
        text = re.sub(f"({marker}[^\n]+)\n(?!\n)", r"\1\n\n", text)
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–æ—Å–ª–µ —Å—Ç—Ä–µ–ª–æ—á–∫–∏ "‚Üí" –≤—Å–µ–≥–¥–∞ –¥–≤–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r"\n‚Üí", "\n\n‚Üí", text)
    # –£–±–∏—Ä–∞–µ–º —Ç—Ä–æ–π–Ω—ã–µ –∏ –±–æ–ª–µ–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    while "\n\n\n" in text:
        text = text.replace("\n\n\n", "\n\n")
    return text.strip()

def force_split_long_string(long_str, limit_b):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —Ä–µ–∂–µ—Ç –¥–ª–∏–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –Ω–∞ —á–∞—Å—Ç–∏, –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—â–∏–µ limit_b –±–∞–π—Ç."""
    sub_chunks = []
    if not long_str:
        return sub_chunks
    
    encoded_str = long_str.encode('utf-8')
    start_idx = 0
    while start_idx < len(encoded_str):
        end_idx = min(start_idx + limit_b, len(encoded_str))
        current_byte_slice = encoded_str[start_idx:end_idx]
        
        # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å, –æ—Ç—Å—Ç—É–ø–∞—è –ø–æ –±–∞–π—Ç—É –Ω–∞–∑–∞–¥ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        while True:
            try:
                decoded_chunk = current_byte_slice.decode('utf-8')
                sub_chunks.append(decoded_chunk)
                start_idx += len(current_byte_slice) # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–∑–∏—Ü–∏–∏
                break 
            except UnicodeDecodeError:
                if len(current_byte_slice) > 1:
                    current_byte_slice = current_byte_slice[:-1] # –£–º–µ–Ω—å—à–∞–µ–º –Ω–∞ 1 –±–∞–π—Ç
                else:
                    # –ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –¥–∞–∂–µ 1 –±–∞–π—Ç. –≠—Ç–æ –∫—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π.
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –±–∞–π—Ç, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞.
                    log(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –±–∞–π—Ç –ø—Ä–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Ä–µ–∑–∫–µ: {encoded_str[start_idx:start_idx+1]!r}")
                    start_idx += 1
                    break # –í—ã—Ö–æ–¥ –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ while, –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≤–Ω–µ—à–Ω–µ–≥–æ
    return sub_chunks

def smart_chunk(text_to_split_paragraphs, outer_limit_bytes):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –±–∞–π—Ç–æ–≤–æ–≥–æ –ª–∏–º–∏—Ç–∞, —Å—Ç–∞—Ä–∞—è—Å—å —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∞–±–∑–∞—Ü—ã."""
    paragraphs = text_to_split_paragraphs.split("\n\n")
    final_chunks = []
    current_chunk_text_parts = []
    current_chunk_accumulated_bytes = 0

    for para_idx, para_str in enumerate(paragraphs):
        if not para_str.strip(): # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∞–±–∑–∞—Ü—ã
            continue

        para_bytes = para_str.encode('utf-8')
        # –ë–∞–π—Ç—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è "\n\n" (2 –±–∞–π—Ç–∞), –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –Ω–µ –ø—É—Å—Ç
        separator_bytes_len = 2 if current_chunk_text_parts else 0 

        if current_chunk_accumulated_bytes + separator_bytes_len + len(para_bytes) <= outer_limit_bytes:
            # –ê–±–∑–∞—Ü –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if current_chunk_text_parts: # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü –≤ —á–∞–Ω–∫–µ
                current_chunk_text_parts.append("\n\n")
            current_chunk_text_parts.append(para_str)
            current_chunk_accumulated_bytes += separator_bytes_len + len(para_bytes)
        else:
            # –ê–±–∑–∞—Ü –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç.
            if current_chunk_text_parts:
                final_chunks.append("".join(current_chunk_text_parts))
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            current_chunk_text_parts = []
            current_chunk_accumulated_bytes = 0

            # –¢–µ–ø–µ—Ä—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 'para_str', –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø–æ–º–µ—Å—Ç–∏–ª—Å—è
            if len(para_bytes) > outer_limit_bytes:
                # –°–∞–º –∞–±–∑–∞—Ü –¥–ª–∏–Ω–Ω–µ–µ –ª–∏–º–∏—Ç–∞, –µ–≥–æ –Ω—É–∂–Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Ä–µ–∑–∞—Ç—å
                log(f"‚ÑπÔ∏è –ê–±–∑–∞—Ü #{para_idx} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(para_bytes)} –±–∞–π—Ç), –±—É–¥–µ—Ç —Ä–∞–∑—Ä–µ–∑–∞–Ω.")
                split_long_paragraph_parts = force_split_long_string(para_str, outer_limit_bytes)
                final_chunks.extend(split_long_paragraph_parts) # –ö–∞–∂–¥—ã–π –∫—É—Å–æ–∫ - –Ω–æ–≤—ã–π —á–∞–Ω–∫
            else:
                # –ê–±–∑–∞—Ü –Ω–µ –¥–ª–∏–Ω–Ω–µ–µ –ª–∏–º–∏—Ç–∞, –Ω–æ –Ω–µ –≤–ª–µ–∑ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫. –ù–∞—á–∏–Ω–∞–µ–º –∏–º –Ω–æ–≤—ã–π —á–∞–Ω–∫.
                current_chunk_text_parts.append(para_str)
                current_chunk_accumulated_bytes = len(para_bytes)
                
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ–±—Ä–∞–Ω–Ω—ã–π —á–∞–Ω–∫, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç
    if current_chunk_text_parts:
        final_chunks.append("".join(current_chunk_text_parts))

    return [chunk for chunk in final_chunks if chunk.strip()] # –£–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏

def send(text_to_send, add_numeration=False):
    prepared_text = prepare_text(text_to_send)
    
    # –ó–∞–ø–∞—Å –±–∞–π—Ç –ø–æ–¥ –ø—Ä–µ—Ñ–∏–∫—Å "–ß–∞—Å—Ç—å XX/YY:\n\n"
    # "–ß–∞—Å—Ç—å 10/10:\n\n" ~ 15 —Å–∏–º–≤–æ–ª–æ–≤. –í UTF-8 —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ 15*4=60 –±–∞–π—Ç, –Ω–æ –æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ.
    # –í–æ–∑—å–º–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø–∞—Å.
    prefix_allowance_bytes = 40 
    
    text_part_limit_bytes = TG_LIMIT_BYTES
    if add_numeration:
        text_part_limit_bytes = TG_LIMIT_BYTES - prefix_allowance_bytes
    
    parts = smart_chunk(prepared_text, text_part_limit_bytes)
    total_parts = len(parts)

    if not parts:
        log("‚ÑπÔ∏è –ù–µ—Ç —á–∞—Å—Ç–µ–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ (—Ç–µ–∫—Å—Ç –ø—É—Å—Ç –∏–ª–∏ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤).")
        return

    for idx, part_content in enumerate(parts, 1):
        final_text_to_send = part_content
        log_message_prefix = "" # –î–ª—è –ª–æ–≥–æ–≤, —á—Ç–æ–±—ã –±—ã–ª–æ –ø–æ–Ω—è—Ç–Ω–æ, –∫–∞–∫–∞—è —á–∞—Å—Ç—å

        if add_numeration and total_parts > 0: # –ù—É–º–µ—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –∏ –µ—Å—Ç—å —á–∞—Å—Ç–∏
            prefix_str = f"–ß–∞—Å—Ç—å {idx}/{total_parts}:\n\n"
            final_text_to_send = prefix_str + part_content
            log_message_prefix = f"–ß–∞—Å—Ç—å {idx}/{total_parts} "
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏ –º—ã –ê–ë–°–û–õ–Æ–¢–ù–´–ô –ª–∏–º–∏—Ç Telegram —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
            final_text_bytes = len(final_text_to_send.encode('utf-8'))
            if final_text_bytes > 4096:
                log(f"üìõ –í–ù–ò–ú–ê–ù–ò–ï! {log_message_prefix}–° –ü–†–ï–§–ò–ö–°–û–ú –°–õ–ò–®–ö–û–ú –î–õ–ò–ù–ù–ê–Ø ({final_text_bytes} –±–∞–π—Ç > 4096). Telegram –û–ë–†–ï–ñ–ï–¢ –≠–¢–£ –ß–ê–°–¢–¨!")
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –∑–¥–µ—Å—å, –Ω–æ —ç—Ç–æ —É—Å–ª–æ–∂–Ω–∏—Ç.
                # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º.

        def send_telegram_request():
            return requests.post(
                f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage",
                json={"chat_id": CHANNEL_ID, "text": final_text_to_send, "disable_web_page_preview": True},
                timeout=10 # –¢–∞–π–º–∞—É—Ç –Ω–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ Telegram
            )

        response = safe_call(send_telegram_request, label=f"‚ùó –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ {log_message_prefix}–≤ TG")
        
        if response and response.status_code == 200:
            log(f"‚úÖ {log_message_prefix}—É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ ({len(final_text_to_send.encode('utf-8'))} –±–∞–π—Ç, {len(final_text_to_send)} —Å–∏–º–≤–æ–ª–æ–≤)")
        elif response:
            log(f"‚ùó –û—à–∏–±–∫–∞ –æ—Ç Telegram –¥–ª—è {log_message_prefix.strip()}: {response.status_code} - {response.text}")
            log(f"   –¢–µ–∫—Å—Ç –ø—Ä–æ–±–ª–µ–º–Ω–æ–π —á–∞—Å—Ç–∏ (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): {final_text_to_send[:100].replace(chr(10), ' ')}")
        else:
            log(f"‚ùó –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å {log_message_prefix.strip()} (–Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞ Telegram).")
            log(f"   –¢–µ–∫—Å—Ç –ø—Ä–æ–±–ª–µ–º–Ω–æ–π —á–∞—Å—Ç–∏ (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): {final_text_to_send[:100].replace(chr(10), ' ')}")

        if total_parts > 1 and idx < total_parts: # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π —á–∞—Å—Ç–µ–π
            sleep(1.5) # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–ª –ø–∞—É–∑—É

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞ ---

def main():
    log("üöÄ –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω.") # –ò–∑–º–µ–Ω–∏–ª —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
    try:
        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ—Ç GPT
        main_report_text = gpt_report()
        
        # 2. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ç—á–µ—Ç–∞
        # –í–∞–∂–Ω–æ, —á—Ç–æ–±—ã –∫–∞–∂–¥–∞—è —á–∞—Å—Ç—å –∏–∑ keyword_alert, store_and_compare, analyze_sentiment
        # —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∞ —Å–≤–æ–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –Ω—É–∂–Ω—ã.
        
        report_components = [
            "üìä –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç",
            main_report_text.strip(), # –¢–µ–∫—Å—Ç –æ—Ç GPT
            
            keyword_alert(main_report_text).strip(), # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É–∂–µ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∏–ø–∞ "üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"
            
            store_and_compare(main_report_text).strip(), # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É–∂–µ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∏–ø–∞ "üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≤—á–µ—Ä–∞"
            
            analyze_sentiment(main_report_text).strip() # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É–∂–µ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∏–ø–∞ "üß† –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
        ]
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ –æ–¥–Ω—É –±–æ–ª—å—à—É—é —Å—Ç—Ä–æ–∫—É —Å –¥–≤–æ–π–Ω—ã–º–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏
        full_report_string = "\n\n".join(filter(None, report_components)) # filter(None, ...) —É–±–µ—Ä–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –∫–∞–∫–∞—è-—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ None –∏–ª–∏ ""

        # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –≤ Telegram
        if full_report_string:
            send(full_report_string, add_numeration=True)
            log("‚úÖ –í–µ—Å—å –æ—Ç—á—ë—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω.")
        else:
            log("‚ÑπÔ∏è –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø—É—Å—Ç, –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")

    except RuntimeError as e: # –û—à–∏–±–∫–∞ –æ—Ç OpenAI
        log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ OpenAI: {e}")
        # –ú–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –≤ Telegram, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        # send(f"üî• –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: –ø—Ä–æ–±–ª–µ–º–∞ —Å OpenAI. {e}", add_numeration=False)
        sys.exit(1) # –í—ã—Ö–æ–¥ —Å –æ—à–∏–±–∫–æ–π, —á—Ç–æ–±—ã Railway –º–æ–≥ —ç—Ç–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å
    except requests.exceptions.RequestException as e:
        log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: {e}")
        log(traceback.format_exc())
        # send(f"üî• –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å–∫—Ä–∏–ø—Ç–∞. {e}", add_numeration=False)
        sys.exit(1)
    except Exception as e:
        log(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –≥–ª–æ–±–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        log(traceback.format_exc())
        # send(f"üî• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ —Ä–∞–±–æ—Ç–µ —Å–∫—Ä–∏–ø—Ç–∞. {e}", add_numeration=False)
        sys.exit(1)

if __name__ == "__main__":
    main()


if __name__ == "__main__":
    main()


