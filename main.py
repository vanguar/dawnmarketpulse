#!/usr/bin/env python3
import nltk
# –ò—Å–ø–æ–ª—å–∑—É–µ–º quiet=True, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ª–∏—à–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –ª–æ–≥–∞—Ö –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)

import os
import sys
import requests
import openai
from datetime import datetime, timezone, date
from time import sleep
import traceback
import re

# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–∏ –º–æ–¥—É–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ç–æ–º –∂–µ –∫–∞—Ç–∞–ª–æ–≥–µ –∏–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ PYTHONPATH
from market_reader import get_market_data_text, get_crypto_data
from news_reader import get_news_block
from analyzer import keyword_alert, store_and_compare
from report_utils import analyze_sentiment

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
openai.api_key = os.getenv("OPENAI_KEY")
TG_TOKEN = os.getenv("TG_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

MODEL = "gpt-4o-mini"
TIMEOUT = 60  # –¢–∞–π–º–∞—É—Ç –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, OpenAI)
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∞–π—Ç–æ–≤—ã–π –ª–∏–º–∏—Ç –¥–ª—è –¢–ï–ö–°–¢–ê –û–î–ù–û–ì–û —Å–æ–æ–±—â–µ–Ω–∏—è (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ "–ß–∞—Å—Ç—å X/Y")
# –≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å, –µ—Å–ª–∏ –æ–±—Ä–µ–∑–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è.
TG_LIMIT_BYTES = 3700 # –ü–æ–ø—Ä–æ–±—É–µ–º —É–º–µ–Ω—å—à–∏—Ç—å –µ—â–µ –Ω–µ–º–Ω–æ–≥–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∑–∞–ø–∞—Å–∞
GPT_TOKENS = 400 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT

GPT_CONTINUATION = """–ê–∫—Ü–∏–∏-–ª–∏–¥–µ—Ä—ã üöÄ / –ê—É—Ç—Å–∞–π–¥–µ—Ä—ã üìâ
- –ø–æ 2‚Äì3 –±—É–º–∞–≥–∏ + –ø—Ä–∏—á–∏–Ω–∞
‚Üí –í—ã–≤–æ–¥.

–ú–∞–∫—Ä–æ-–Ω–æ–≤–æ—Å—Ç–∏ üì∞
- 3 –≥–ª–∞–≤–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞ + –≤–ª–∏—è–Ω–∏–µ

–¶–∏—Ç–∞—Ç—ã –¥–Ω—è üó£
- –¥–æ 2 —Ü–∏—Ç–∞—Ç + —Å–º—ã—Å–ª

–ß–∏—Å–ª–æ-—Ñ–∞–∫—Ç ü§î

‚ö°Ô∏è –ò–¥–µ—è –¥–Ω—è ‚Äì 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è actionable-—Å–æ–≤–µ—Ç–∞.

‚ÄºÔ∏è –¢–æ–ª—å–∫–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç, –±–µ–∑ HTML.
‚ÄºÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π —Ç–µ–∫—Å—Ç —Å –î–í–û–ô–ù–´–ú–ò –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫ –º–µ–∂–¥—É –∞–±–∑–∞—Ü–∞–º–∏.
‚ÄºÔ∏è –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —Ä–∞–∑–¥–µ–ª–æ–≤."""

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def log(msg):
    timestamp = f"[{datetime.now(timezone.utc):%Y-%m-%d %H:%M:%S} UTC]"
    print(f"{timestamp} {msg}", flush=True)

def safe_call(func, retries=3, delay=5, label="‚ùó –û—à–∏–±–∫–∞"):
    for i in range(retries):
        try:
            return func()
        except requests.exceptions.Timeout:
            log(f"{label}: –ø–æ–ø—ã—Ç–∫–∞ {i + 1}/{retries} –Ω–µ —É–¥–∞–ª–∞—Å—å - –¢–∞–π–º–∞—É—Ç ({TIMEOUT}—Å)")
            if i < retries - 1:
                sleep(delay)
        except requests.exceptions.RequestException as e: # –ë–æ–ª–µ–µ –æ–±—â–∞—è –æ—à–∏–±–∫–∞ —Å–µ—Ç–∏
            log(f"{label}: –ø–æ–ø—ã—Ç–∫–∞ {i + 1}/{retries} –Ω–µ —É–¥–∞–ª–∞—Å—å - –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
            if i < retries - 1:
                sleep(delay)
        except Exception as e:
            log(f"{label}: –ø–æ–ø—ã—Ç–∫–∞ {i + 1}/{retries} –Ω–µ —É–¥–∞–ª–∞—Å—å - –û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")
            log(traceback.format_exc()) # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π traceback –¥–ª—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
            if i < retries - 1:
                sleep(delay)
    log(f"{label}: –≤—Å–µ {retries} –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–µ–Ω—ã.")
    return None

# --- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ ---

def gpt_report():
    today = date.today().strftime("%d.%m.%Y")
    header = f"üìÖ –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ {today}"
    
    market_data_text = get_market_data_text()
    crypto_data_text = get_crypto_data(extended=True)
    news_block_text = get_news_block() # get_news_block —É–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT

    dynamic_data = (
        f"{header}\n\n"
        f"{market_data_text}\n\n"
        f"{crypto_data_text}\n\n"
        f"{news_block_text}\n\n" 
        f"{GPT_CONTINUATION}" 
    )
    
    log(f"‚ÑπÔ∏è –î–∞–Ω–Ω—ã–µ –¥–ª—è GPT (–¥–ª–∏–Ω–∞): {len(dynamic_data)} —Å–∏–º–≤–æ–ª–æ–≤. –ü–µ—Ä–≤—ã–µ 200: {dynamic_data[:200]}...")

    response = safe_call(
        lambda: openai.ChatCompletion.create(
            model=MODEL,
            messages=[{"role": "user", "content": dynamic_data}],
            timeout=TIMEOUT, # –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI
            temperature=0.4,
            max_tokens=GPT_TOKENS,
        ),
        label="‚ùó –û—à–∏–±–∫–∞ OpenAI"
    )
    if not response:
        raise RuntimeError("OpenAI –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")
    
    generated_text = response.choices[0].message.content.strip()
    log(f"üìù GPT —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç ({len(generated_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
    return generated_text

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ Telegram ---

def prepare_text(text_to_prepare):
    if not isinstance(text_to_prepare, str): # –î–æ–±–∞–≤–∏–º –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–∏–ø–∞
        log(f"‚ö†Ô∏è prepare_text –ø–æ–ª—É—á–∏–ª –Ω–µ —Å—Ç—Ä–æ–∫—É: {type(text_to_prepare)}. –í–æ–∑–≤—Ä–∞—â–∞—é –∫–∞–∫ –µ—Å—Ç—å.")
        return str(text_to_prepare) # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å—Ç—Ä–æ–∫—É –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π

    for marker in ["üìä", "üöÄ", "üìâ", "‚Çø", "üì∞", "üó£", "ü§î", "‚ö°Ô∏è", "üîç", "üìà", "üß†"]:
        text_to_prepare = re.sub(f"({marker}[^\n]*)\n(?!\n)", r"\1\n\n", text_to_prepare)
    
    text_to_prepare = re.sub(r"(\n‚Üí[^\n]*)\n(?!\n)", r"\1\n\n", text_to_prepare) 
    text_to_prepare = re.sub(r"(\n‚Üí)$", r"\1\n", text_to_prepare) 

    while "\n\n\n" in text_to_prepare:
        text_to_prepare = text_to_prepare.replace("\n\n\n", "\n\n")
    return text_to_prepare.strip()


def force_split_long_string(long_str, limit_b):
    sub_chunks = []
    if not long_str: 
        return sub_chunks
    
    encoded_str = long_str.encode('utf-8')
    current_byte_pos = 0
    while current_byte_pos < len(encoded_str):
        end_byte_pos = min(current_byte_pos + limit_b, len(encoded_str))
        byte_slice_candidate = encoded_str[current_byte_pos:end_byte_pos]
        
        while True:
            try:
                decoded_chunk = byte_slice_candidate.decode('utf-8')
                sub_chunks.append(decoded_chunk)
                current_byte_pos += len(byte_slice_candidate) 
                break 
            except UnicodeDecodeError:
                if len(byte_slice_candidate) > 1:
                    byte_slice_candidate = byte_slice_candidate[:-1] 
                else:
                    log(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º—ã–π –±–∞–π—Ç –ø—Ä–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Ä–µ–∑–∫–µ: {encoded_str[current_byte_pos:current_byte_pos+1]!r}")
                    current_byte_pos += 1 
                    break 
    return sub_chunks


def smart_chunk(text_to_chunk, outer_limit_bytes):
    paragraphs = text_to_chunk.split("\n\n") 
    final_result_chunks = []
    current_accumulated_parts = [] 
    current_accumulated_bytes = 0  

    for para_idx, paragraph_str in enumerate(paragraphs):
        if not paragraph_str.strip(): 
            continue

        paragraph_bytes = paragraph_str.encode('utf-8')
        separator_bytes_len = 2 if current_accumulated_parts else 0 

        if current_accumulated_bytes + separator_bytes_len + len(paragraph_bytes) <= outer_limit_bytes:
            if current_accumulated_parts: 
                current_accumulated_parts.append("\n\n")
            current_accumulated_parts.append(paragraph_str)
            current_accumulated_bytes += separator_bytes_len + len(paragraph_bytes)
        else:
            if current_accumulated_parts:
                final_result_chunks.append("".join(current_accumulated_parts))
            
            current_accumulated_parts = []
            current_accumulated_bytes = 0

            if len(paragraph_bytes) > outer_limit_bytes:
                log(f"‚ÑπÔ∏è –ê–±–∑–∞—Ü #{para_idx} '{paragraph_str[:30].replace(chr(10),' ')}...' —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(paragraph_bytes)} –±–∞–π—Ç > {outer_limit_bytes} –±–∞–π—Ç), –±—É–¥–µ—Ç —Ä–∞–∑—Ä–µ–∑–∞–Ω.")
                split_long_paragraph_sub_chunks = force_split_long_string(paragraph_str, outer_limit_bytes)
                final_result_chunks.extend(split_long_paragraph_sub_chunks) 
            else:
                current_accumulated_parts.append(paragraph_str)
                current_accumulated_bytes = len(paragraph_bytes)
                
    if current_accumulated_parts:
        final_result_chunks.append("".join(current_accumulated_parts))

    return [chunk_item for chunk_item in final_result_chunks if chunk_item.strip()]


def send(text_content, add_numeration_if_multiple_parts=False):
    prepared_text_content = prepare_text(str(text_content)) # –î–æ–±–∞–≤–∏–ª str() –¥–ª—è –±–æ–ª—å—à–µ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    
    prefix_max_allowance_bytes = 40 
    
    text_chunk_actual_limit_bytes = TG_LIMIT_BYTES 
    
    # –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –Ω–∞—Ä–µ–∑–∫—É, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç —á–∞—Å—Ç–µ–π
    # –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è, –ø—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç –ø–æ–¥ –ø—Ä–µ—Ñ–∏–∫—Å.
    # –ï—Å–ª–∏ –Ω—É–º–µ—Ä–∞—Ü–∏—è –Ω—É–∂–Ω–∞ (—Ç.–µ. —á–∞—Å—Ç–µ–π –±—É–¥–µ—Ç > 1), —Ç–æ –ª–∏–º–∏—Ç –¥–ª—è smart_chunk –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–Ω—å—à–µ.
    limit_for_pre_chunking = TG_LIMIT_BYTES
    if add_numeration_if_multiple_parts: # –ï—Å–ª–∏ –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –Ω—É–º–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è
        limit_for_pre_chunking = TG_LIMIT_BYTES - prefix_max_allowance_bytes
        
    parts_list = smart_chunk(prepared_text_content, limit_for_pre_chunking)
    total_parts_count = len(parts_list)

    # –ï—Å–ª–∏ –Ω—É–º–µ—Ä–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å, –Ω–æ —á–∞—Å—Ç–µ–π –≤—Å–µ–≥–æ –æ–¥–Ω–∞, —Ç–æ –¥–ª—è —ç—Ç–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —á–∞—Å—Ç–∏
    # –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π TG_LIMIT_BYTES, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–µ—Ñ–∏–∫—Å–∞ "–ß–∞—Å—Ç—å 1/1" –Ω–µ –±—É–¥–µ—Ç.
    if add_numeration_if_multiple_parts and total_parts_count == 1:
        parts_list = smart_chunk(prepared_text_content, TG_LIMIT_BYTES) # –ü–µ—Ä–µ–Ω–∞—Ä–µ–∑–∞–µ–º —Å –ø–æ–ª–Ω—ã–º –ª–∏–º–∏—Ç–æ–º
        total_parts_count = len(parts_list) # –î–æ–ª–∂–Ω–æ –æ—Å—Ç–∞—Ç—å—Å—è 1, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫

    if not parts_list:
        log("‚ÑπÔ∏è –ù–µ—Ç —á–∞—Å—Ç–µ–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ (—Ç–µ–∫—Å—Ç –ø—É—Å—Ç –∏–ª–∏ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤).")
        return

    for idx, single_part_content in enumerate(parts_list, 1):
        final_text_for_telegram = single_part_content
        log_part_prefix = "" 

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é "–ß–∞—Å—Ç—å X/Y", —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á–∞—Å—Ç–µ–π –ë–û–õ–¨–®–ï –û–î–ù–û–ô –∏ —Ñ–ª–∞–≥ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        if add_numeration_if_multiple_parts and total_parts_count > 1:
            numeration_prefix_str = f"–ß–∞—Å—Ç—å {idx}/{total_parts_count}:\n\n"
            final_text_for_telegram = numeration_prefix_str + single_part_content
            log_part_prefix = f"–ß–∞—Å—Ç—å {idx}/{total_parts_count} " 
            
            final_text_bytes_with_prefix = len(final_text_for_telegram.encode('utf-8'))
            if final_text_bytes_with_prefix > 4096:
                log(f"üìõ –í–ù–ò–ú–ê–ù–ò–ï! {log_part_prefix}–° –ü–†–ï–§–ò–ö–°–û–ú –°–õ–ò–®–ö–û–ú –î–õ–ò–ù–ù–ê–Ø ({final_text_bytes_with_prefix} –±–∞–π—Ç > 4096). Telegram –û–ë–†–ï–ñ–ï–¢ –≠–¢–£ –ß–ê–°–¢–¨!")

        def make_telegram_api_call():
            return requests.post(
                f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage",
                json={"chat_id": CHANNEL_ID, "text": final_text_for_telegram, "disable_web_page_preview": True},
                timeout=15 
            )

        response_from_tg = safe_call(make_telegram_api_call, label=f"‚ùó –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ {log_part_prefix}–≤ TG")
        
        current_part_final_bytes = len(final_text_for_telegram.encode('utf-8'))
        current_part_final_chars = len(final_text_for_telegram)

        if response_from_tg and response_from_tg.status_code == 200:
            log(f"‚úÖ {log_part_prefix}—É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ ({current_part_final_bytes} –±–∞–π—Ç, {current_part_final_chars} —Å–∏–º–≤–æ–ª–æ–≤)")
        elif response_from_tg:
            error_text_preview = final_text_for_telegram[:150].replace('\n', ' ') 
            log(f"‚ùó –û—à–∏–±–∫–∞ –æ—Ç Telegram –¥–ª—è {log_part_prefix.strip()}: {response_from_tg.status_code} - {response_from_tg.text}")
            log(f"   –¢–µ–∫—Å—Ç –ø—Ä–æ–±–ª–µ–º–Ω–æ–π —á–∞—Å—Ç–∏ (–±–∞–π—Ç—ã: {current_part_final_bytes}, —Å–∏–º–≤–æ–ª—ã: {current_part_final_chars}, –Ω–∞—á–∞–ª–æ): '{error_text_preview}...'")
        else: 
            error_text_preview = final_text_for_telegram[:150].replace('\n', ' ')
            log(f"‚ùó –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å {log_part_prefix.strip()} (–Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞ Telegram –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫).")
            log(f"   –¢–µ–∫—Å—Ç –ø—Ä–æ–±–ª–µ–º–Ω–æ–π —á–∞—Å—Ç–∏ (–±–∞–π—Ç—ã: {current_part_final_bytes}, —Å–∏–º–≤–æ–ª—ã: {current_part_final_chars}, –Ω–∞—á–∞–ª–æ): '{error_text_preview}...'")

        if total_parts_count > 1 and idx < total_parts_count: 
            sleep_duration = 1.5 
            log(f"‚ÑπÔ∏è –ü–∞—É–∑–∞ {sleep_duration} —Å–µ–∫. –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π —á–∞—Å—Ç—å—é...")
            sleep(sleep_duration)

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞ ---

def main():
    log("üöÄ –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω.")
    try:
        main_report_text_from_gpt = gpt_report() 
        
        list_of_report_components = [
            "üìä –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç", 
            main_report_text_from_gpt, # –£–∂–µ .strip() –∏–∑ gpt_report()
            
            keyword_alert(main_report_text_from_gpt), 
            
            store_and_compare(main_report_text_from_gpt), 
            
            analyze_sentiment(main_report_text_from_gpt) 
        ]
        
        # –£–±–∏—Ä–∞–µ–º None –∏–ª–∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏/—Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –∑–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ–º
        valid_components = []
        for component in list_of_report_components:
            if isinstance(component, str) and component.strip():
                valid_components.append(component.strip()) # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ strip() –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
            elif component is not None: # –ï—Å–ª–∏ –Ω–µ —Å—Ç—Ä–æ–∫–∞, –Ω–æ –Ω–µ None, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                log(f"‚ö†Ô∏è –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –æ—Ç—á–µ—Ç–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–æ–π, –Ω–æ –Ω–µ None: {type(component)}. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ —Å—Ç—Ä–æ–∫—É.")
                str_component = str(component).strip()
                if str_component:
                    valid_components.append(str_component)
        
        full_report_final_string = "\n\n".join(valid_components)

        if full_report_final_string:
            send(full_report_final_string, add_numeration_if_multiple_parts=True)
            log("‚úÖ –í–µ—Å—å –æ—Ç—á—ë—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω.")
        else:
            log("‚ÑπÔ∏è –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø—É—Å—Ç, –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")

    except RuntimeError as e: 
        log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ GPT-–æ—Ç—á–µ—Ç–∞: {e}")
        sys.exit(1) 
    except requests.exceptions.RequestException as e: 
        log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")
        log(traceback.format_exc())
        sys.exit(1)
    except Exception as e: 
        log(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –≥–ª–æ–±–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}")
        log(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()